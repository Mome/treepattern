\documentclass[english]{article}
\usepackage[utf8]{inputenc}
\usepackage{babel}
\usepackage[linesnumbered]{algorithm2e}
\usepackage{newlfont}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{tgcursor}
\usepackage{glossaries}
\usepackage{rotating}
\usepackage{adjustbox}
\usepackage{mdframed}
\usepackage{pdflscape}
\usepackage{verbatim}
\usepackage{amsthm}
\usepackage{float}

\newtheorem*{definition}{Definition}

\graphicspath{ {images/} }
\lstset{basicstyle=\ttfamily}

\begin{document}

\title{A Domain-Specific Language for Graph Generation from Parse Trees by Pattern Matching}


\author{Moritz Meier}

\maketitle

\section{Introduction}

Information extraction (IE) from written natural language remains, due to the immense complexity and ambiguity of human language, an unsolved problem. Tree and graph transformation has recently gained some attention as a natural language methods\cite{ribeyre_linguistically-motivated_2012}. This text descries a domain-specific language (DSL) and its underlying algorithm, which were designed to transform a phrase structure parse-tree into a graph of semantic relations. The software can in principle be used on any tree structure to produce other kinds of graph structures. The method also includes that additional properties can be assigned to the tree structure, such that information of other IE sources, can be incorporated into one single representation. The DSL makes it convenient to define pairs of local tree patterns and its corresponding transitions to create and transform nodes and edges of a graph. A working implementation can be downloaded from \url{http://github.com/mome/treepattern}. In case the reader is unfamiliar with abbreviations for constituents, used in this text, here is a list of all tags used in the Stanford Parser: \url{http://github.com/mome/treepattern/wiki/List-of-Constituent-Tags}.


\subsection{Motivation}

If we consider natural language as a \textit{serialization of mental content}, which is decoded and encoded by humans in order to communicate. If we also define a formal language that can partially express mental content, then there must be a method which can partially map natural language to the formal language. Further on we can see that natural language has a (mostly) modular architecture. Words can be composed syntactically to form a unit with a composed meaning. Linguistics calls this composable units \textit{constituents}, but in different theories different parts of a sentence qualify as a constituent.

Phrase-structure grammars are a common way assign a hierarchical structure of constituents to a sentence. They are context-free grammars that yield a parse tree with words as terminal symbols and nodes referring to constituents of the sentence. Despite being a syntactic method, phrase-structure parse trees also carry semantic information. Constituents can refer to entities or concepts, others express relations between concepts. How is it possible to extract this semantic information?

Computational linguistics possesses various other isolated methods to provide information about the parts of a sentence. POS-tagging finds the syntactic role of words, lemmatization removes inflection, NER-tagging assigns nouns to (semantic) categories. How it is possible to bring these different sources together and put them in one unified representation?


\subsection{Related Work}

Pattern Matching in trees is used in various different contexts. \cite{hoffmann_pattern_1982} is a detailed discussion on pattern matching in programming language parse trees and subtree replacement for such things as code optimization and theorem proving. \cite{rim_transforming_1990} describes a computational method to transform syntactic graphs into semantic ones. The primary function of TGrep \cite{rohde_tgrep2_2004} is to extract parse trees whose structures match a specified pattern. In \cite{de_marneffe_generating_2006} dependency parse trees are generated from phrase structure parse trees.
The python package \textit{ptTools} also defines a DSL for pattern matching in trees, which is intended to be applied on python parse trees for code analysis and testing. Tregex is a tool for tree querying and Tsurgen for tree manipulation \cite{levy_tregex_2006}. In \cite{ribeyre_linguistically-motivated_2012} graphs are created from dependency parse trees.

\paragraph{XPath}
The XML Path Language (or short XPath) is a query language for node selection in XML documents. The version XPath 3.0 is currently a WC3 Recommendation and in wide use. The following descriptions are respective to XPath 1.0.

\begin{comment}
Figure \ref{fig:xpath} shows eleven of the thirteen different relationships that can be expressed in XPath. All relationships are defined relative to a node which is addressed by \textit{self}. On the vertical axis there is \textit{parent}, \textit{ancestor} and \textit{ancestor-or-self} in the upward direction and \textit{child}, \textit{descendant} and \textit{descendant-or-self} in the downward direction. On the horizontal axis there is \textit{preceding} and \textit{preceding-sibling} to the left and \textit{following} and \textit{following-sibling} to the right.
\end{comment}

Figure \ref{fig:xpath} visualizes the basic relationships that can be expressed
in XPath in order to construct more complex patterns. Patterns can
be matched on two axis, relative to a matching node denoted as \textit{self}.
On the vertical axis, a higher level node can be matched as \textit{ancestor},
a lower as \textit{descendant}. If the \textit{self} node should be included, \textit{ancestor-or-self} or \textit{descendant-or-self} can be used. A ancestor of first degree is a \textit{parent}
and a decedent of first degree a \textit{child}. On the horizontal
axis \textit{preceding} and \textit{following} nodes can be matched.
If a preceding or following node also happens to be a sibling, it can be
matched with \textit{preceding-sibling} and \textit{following-sibling}.

\begin{figure}
\centering
\includegraphics[scale=0.35]{xpath-axis}
\caption{Subset of relationships expressible in XPath}
\label{fig:xpath}
\end{figure}


\subsection{The Immediate-Following Relationship}

Another criterion for the language design was, being able to define the patterns in the same way as Hearst-Patterns \cite{hearst_automatic_1992} are defined. The first pattern from the original publication looks like this:

\[ NP_0\ such\ as\ \{NP_1,\ NP_2\ ...\ ,(and|or)\}\ NP_n \]

In this representation words and constituency tags are just put in a row separated by whitespace. While this looks very intuitive to a human, the technical formulation of the underlying relation, which is crucial for an actual implementation, requires some explanation.

\label{ssec:imidiate-following}

\begin{figure}
\centering
\includegraphics[scale=0.5]{lara_drinks_tea}
\caption{Phrase-Structure Parse Tree: Yellow nodes have no preceding nodes, blue nodes are immediate followers of the yellow marks NP node.}
\label{fig:lara-drinks-tea}
\end{figure}


Figure \ref{fig:lara-drinks-tea} shows a phrase-structure parse tree of the sentence: "Lara drinks tea.". Lets try to match two simpler patterns in this Hearst-Pattern style on this tree:
\[NP_0\ drinks\ NP_1\ .\]
\[NP\ \ VP\ \ tea\ .\]
How to decide which one matches the sentence and which one does not?  

The nodes that can be in the beginning of a pattern are exactly the once that have no predecessor (yellow nodes in Figure \ref{fig:lara-drinks-tea}). Based on the XPath naming scheme, these nodes are said to be in a \textit{no-preceding} relation to the root node (here \textbf{S}). Both patterns would assign the yellow marked noun phrase node to their first pattern token. Once the NP node is fixed, he next possible nodes to match are: \textbf{VP}, \textbf{VBZ} or \textbf{drinks} (marked with blue). In general the constituents that can stand at the next position in a pattern hold a relation called \textit{immediate-following} to the previous constituent.

\begin{definition}[imidiate-following]
 If a node has preceding siblings, then the first preceding sibling is immediate-following. If a node has no preceding siblings then the first preceding sibling of the closest ancestor that has preceding siblings is immediate-following. The first child of an immediate-following node is immediate-following.
\end{definition}

In the first pattern, $drinks$ would match to the \textbf{drinks} terminal node. The token $NP_1$ would consequently match to the second \textbf{NP} node and at the end the dot token to the punctuation constituent node. The whole pattern matches. In the second pattern the token $VP$ matches to the node \textbf{VP}. The set immediate-following nodes for \textbf{VP} only contains the punctuation nodes. Hence the next token $tea$ cannot match, so the whole pattern does not match.


\section{The Domain Specific Language}

Before the pattern matching can begin a representation of the sentence structure, called the \textit{property tree}, which incorporates different sources of information is created. It is based on the phrase-structure parse tree with additional properties assigned to some node. The nodes of the property tree are also just referred to as constituents. For example, if the named-entity of a word is 'person', a new variable with name 'ner' and value 'person' can be created and added to the corresponding constituent.

After that a set of rules is loaded the way how patterns of constituents are translated into semantic domain. A rule consists of four parts, each separated by a colon: 

\[ \mathtt{root\text{-}pattern\ :\ pattern\ :\ relations\ :\ transformation} \] 

The \textit{root-pattern} is a single and the \textit{pattern} is a series of \textit{pattern-tokens}. A pattern-token defines a set of constraints in order to match a single node or in special cases multiple nodes (see Section \ref{para:multi}) of the property tree.
A node that is matched by the root-pattern will form the root node of a subtree (of the property tree). The pattern is written in a Hearst-Pattern style (just like in Section \ref{ssec:imidiate-following}) and can match constituents inside the subtree stretched by the root-pattern. Constituents matched by two adjacent pattern-tokens must stand in a immediate-following relationship. In the simplest case the pattern token can  be the label of a constituent (for example $\mathtt{NP}$) or a terminal (for example $\mathtt{drinks}$). The algorithm automatically adds 'label=NP' for constituents and 'terminal=drinks' for terminals to the constraints set. If the same name occurs more than ones, numbers can be added to the token (for example $\mathtt{NP1}$). The numbers will just effect the variable name of the pattern-token, not the constraint set.  
The \textit{relations} part defines new edges of a graph. The variable names of the pattern-tokens are hereby reused. $\mathtt{NP-drinks}$ will create a directed edge between a node associated to constituent matched by $\mathtt{NP}$ and a node associated to constituents matched by $\mathtt{drinks}$.
The \textit{transformation} part assigns a head-constituents to the constituent matched by the root-pattern. It consists usually of one pattern-token variable name (in special cases more, see head-splitting). If another rule tries to create an edge pointing the the constituent matched by the root-pattern, it will instead point to the head-constituent.
Once all rules are parsed the constituent tags of the graph are replaced by the corresponding terminal strings.
\begin{figure}
\centering
\includegraphics[width=.6\linewidth]{step_example_parsetree}
\lstset{
  %caption=Descriptive Caption Text,
  numbers=left,
  frame=tb,
  xleftmargin=.15\textwidth, xrightmargin=.15\textwidth
} 
\begin{lstlisting}
 S  : NP VP .  : NP-VP      : S
 VP : V  NP PP : V-NP  PP-V : V
 PP : IN DT N  : DT-N  N-IN : IN 
 DT : a :: indefinite
\end{lstlisting}
\caption{Parse tree and rules for table \ref{tab:step-by-step}}
\label{fig:step-by-step}
\end{figure}
Table \ref{tab:step-by-step} gives a step-by-step example of the whole matching and graph-building process applied. The parse tree and rules hereby used are can be seen in Figure \ref{fig:step-by-step}. Column one shows which constituents (bold font) matched to which pattern-token (typewriter font). Column two show the variable graph at that point. Column two show the actual resulting graph, if the terminal assignment would already be performed at that point.


\paragraph{Macros}{
  Macros are syntactic sugar for matching multiple options.  The list below contains all macros currently in the system:

  \begin{lstlisting}
  V  = {VB, VBD, VBG, VBN, VBP, VBZ}
  N  = {NN, NNS, NNP, NNPS, PRP}
  W  = {WHADJP, WHAVP, WHNP, WHPP}
  SS = {S, SBAR, SBARQ, SINV, SQ}
  J  = {JJ, JJR, JJS}
  \end{lstlisting}

  With \lstinline{V} standing for verbs, \lstinline{N} for nouns, \lstinline{W} for questions, \lstinline{SS} for sentences and \lstinline{J} for adjectives. They can be used just as other variables. 
}

\paragraph{Property matching}
Matching of other properties than 'terminal' or 'label' is expressed in squared brackets after the variable name. Write a variable name and a value separated by an equal sign to check if a node has a property with that name and value. The expression \lstinline{N[ner=person]} matches nouns with the NER-tag 'person'. Write only the variable name to check if a node has a property with that name. \lstinline{J[color]} matches adjectives with the property 'color'. Multiple constraints are separated by a comma: \lstinline{N[ner=person | gender=female]}.
The right hand side of a property constraint can also be a set values. In this case a constituent must have one of these properties to match. Example: \lstinline|DT[terminal={an,a}]| - matches a determiner with terminal 'a' or 'an'.

\paragraph{Multiple and Optional parsing}
\label{para:multi}
Variables defined in pattern can occur multiple times to match multiple constituents. The relations defined for that variable are consequently applied to all matches. Example: The rule \lstinline{S : N V and V . : N-V : S} 
contains two time the variable \lstinline{V}. The rule applied to "Hanna walks and talks." will match 'walks' as well as 'talks' for \lstinline{V}. So the relation \lstinline{N-V} is formed two times, resulting in: (Hanna)$\rightarrow$ [walks], and (Hanna)$\rightarrow$ [talks].
Three postfix operators, inspired by regular expressions, can be added to the end of a pattern token, to make the match optional or repetitive. Multiple nodes assigned to a variable are handled in same way as before.

\begin{tabular}{cll}
  $\mathbf{?}$ & optional & match once or do not match\\
  $+$ & multiple & match at least once\\
  $*$ & multinal & match arbitrary times (including: do not match)
\end{tabular}

Example: \lstinline{NP : DT? J* N : J-N : N} matches a noun with or with or without a preceding determiner and arbitrary adjectives.


\paragraph{Cluster references}
If a sentence contains a dependent clause, one part of the sentence reference to another part of the sentence. For this purpose it is possible to specify a subgraph instead of a node as the head. If the root of a rule is equal to its transitions, the entities referring to the ancestors of a matching constituent will be put into a cluster. If another entity wants to point to the entity representing the root match, it will point to the cluster instead.

\paragraph{Head Splitting}
For some sentences it is desirable to let a node point to multiple other nodes. Consider this conjunction of two verb phrases: "Bob runs and cries."

\paragraph{Predicate Mode}
In predicate mode the relations part of a rule will define predicates instead of edges, with a slide changed of syntax. Example: 

\begin{lstlisting}
S : NP1 is mother of NP2 : mother_of(NP1,NP2) : S
\end{lstlisting}

\begin{landscape}
\begin{table}
  \centering
  \begin{tabular}{|c|c|c|}
    \hline  
    Pattern Match & Graph with Constituent Labels & Graph with Terminal Labels \\ \hline \hline 

    \begin{tabular}{c||c|c|c}
      $\mathtt{S}$ & $\mathtt{NP}$ & $\mathtt{V}$ & . \\ \hline
      \textbf{S} & \textbf{NP}\textsubscript{1} & \textbf{V} & \textbf{.} 
    \end{tabular} &
    \adjustbox{valign=c}{\includegraphics[scale=0.5]{step_example_raw_1}} &
    \adjustbox{valign=c}{\includegraphics[scale=0.4]{step_example_1}} \\ \hline

    \begin{tabular}{c||c|c|c}
      $\mathtt{VP}$ & $\mathtt{V}$ & $\mathtt{NP}$ & $\mathtt{PP}$ \\ \hline
      \textbf{VP} & \textbf{VBP} & \textbf{NP}\textsubscript{2} & \textbf{PP}
    \end{tabular} &
    \adjustbox{valign=c}{\includegraphics[scale=0.5]{step_example_raw_2}} &
    \adjustbox{valign=c}{\includegraphics[scale=0.4]{step_example_2}}\\ \hline

    \begin{tabular}{c||c|c|c}
      $\mathtt{PP}$ & $\mathtt{IN}$ & $\mathtt{DT}$ & $\mathtt{N}$  \\ \hline
      \textbf{PP} & \textbf{IN} & \textbf{DT}  & \textbf{NN} 
    \end{tabular} &
    \adjustbox{valign=c}{\includegraphics[scale=0.5]{step_example_raw_3}}&
    \adjustbox{valign=c}{\includegraphics[scale=0.4]{step_example_3}}\\ \hline

    \begin{tabular}{c||c}
      $\mathtt{DT}$ & $\mathtt{a}$ \\ \hline
      \textbf{DT} & a
    \end{tabular} &
    \adjustbox{valign=c}{\includegraphics[scale=0.5]{step_example_raw_4}}&
    \adjustbox{valign=c}{\includegraphics[scale=0.4]{step_example_4}}\\ \hline
  \end{tabular}
  \caption{Step-by-step application of the rules from figure \ref{fig:step-by-step}}
  \label{tab:step-by-step}
\end{table}
\end{landscape}
\begin{landscape}
\centering
\begin{minipage}{0.45\linewidth}%
  \centering
  \begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{cluster_reference_parsetree}
  \begin{lstlisting}[frame=tb, numbers=left, keepspaces=true, xleftmargin=.1\textwidth, xrightmargin=.1\textwidth]]
SS : NP VP .? : NP-VP : SS
SS : IN SS1   :       : SS1
VP : V SS     : V-SS  : V\end{lstlisting}
  \includegraphics[width=1\linewidth]{cluster_reference_graph}
  \caption{\textbf{Cluster References.} Root and tail of line 1 are both SS: ... Line 2: "that" is omitted. Line 3: make an edge between the verb of the first part ("said") and the dependent clause "Lara smokes". }
  \label{fig:cluster-ref}
  \end{figure}
\end{minipage}%
\hspace{0.05\linewidth}
\begin{minipage}{0.45\linewidth}%
  \centering
  \begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{headsplit_parsetree}
  \begin{lstlisting}[frame=tb, keepspaces=true, xleftmargin=.01\textwidth, xrightmargin=.01\textwidth] 
SS : NP  VP .   : NP-VP : SS
VP : VP1 CC VP2 :       : VP1 VP2\end{lstlisting}
  \includegraphics[width=.4\linewidth]{headsplit_graph}
  \caption{\textbf{Head Splitting.} The rules in the middle applied to the parse tree on the top yield the graph in the bottom.}
  \label{fig:head-splitting}
  \end{figure}
\end{minipage}
\end{landscape}

\section{Algorithm}

In the implementation the matching algorithm consists of an outer function \textit{MatchPattern} which the recursive inner function \textit{FindAssignment}.

In line 2 and 3 of MatchPattern only nodes that match the root-pattern are selected. In line 4 selects the nodes that can match the first token of a pattern. Finally in line 5 FindAssignments will fill the list with nodes of the property tree.
\begin{algorithm}
\SetAlgoLined
\SetKwProg{Fn}{Function}{:}{end}
\SetKwFunction{MatchPattern}{MatchPattern}
\SetKwFunction{FindMatches}{FindMatches}
\SetKwData{root}{root}
\SetKwData{head}{head}
\SetKwData{pattern}{pattern}
\SetKw{in}{in}
\Fn{\MatchPattern(\root, \head, \pattern)}
{

  \ForEach{node \in \root.descendant\_or\_self}{
    \If{node satisfies head constraints}
    {
      start\_nodes $\leftarrow$ node.no\_preceding

      matches $\leftarrow$ \FindMatches(start\_nodes, \pattern)  

      return matches
    }
  }

}
\end{algorithm}

In line 2 and 3 of FindAssignment assignment becomes an empty list, where the variable assignments created from the pattern tokens, can be stored. Line 5 is the recursion stop. If all variables have an assignment and if there are nodes immediate-followers to the last matching node, the current assignment is a match. Line 8 pics the data from the current pattern-token. Line 9 and 10 selects all nodes that match the pattern-token (aka have all properties required by the constraints). Line 11 and 12: If a node matches, that node is assigned to the variable and FindAssignment is called for next pattern-token again.

\begin{algorithm}
\SetAlgoLined
\SetKwProg{Fn}{Function}{:}{end}
\SetKwFunction{MatchPattern}{MatchPattern}
\SetKwFunction{FindAssignment}{FindAssignment}
\SetKwData{root}{root}
\SetKwData{head}{head}
\SetKwData{pattern}{pattern}
\SetKw{in}{in}
\Fn{\FindAssignment(next\_nodes, pattern, assignment=None, index=0)}
{
  \If{assignment == None}{
    assignment $\leftarrow$ empty list of length(pattern)
  }
  \eIf{(len(pattern) == index) and (len(nodes) == 0)}
  {
    yield copy(assignment)
  }{
    varname, constraints $\leftarrow$ pattern[index]

    \ForEach{node \in next\_nodes}{
      \If{node satisfies constraints}{
        assignment[index] $\leftarrow$ (varname, node)

        matches = FindAssignment(node.imidiate\_following, pattern, assignment, index+1)

        yield from matches
      }
    }
  } 
}
\end{algorithm}

\begin{comment}
    else:
        varname, constraints = pattern[index]

        for node in nodes:

            satisfies_constraints = node.has_properties(**constraints)

            if not satisfies_constraints:
                continue

            match[index] = (varname, node) # assign node to variable

            immediate_following = node.immediate_following(root=head_node)

            yield from self._search(immediate_following, pattern, head_node, match, index+1)

\end{comment}

\section{Example: Hearst-Pattern}

The exact form of the first Hearst-Pattern in its original publication is:

\[
NP_0\ such\ as\ \{NP_1,\ NP_2\ ...\ ,(and|or)\}\ NP_n
\]

Expressed in the DSL this looks like:

\[
\mathtt{NP\ :\ NP0\ such\ as\ ((N\ ,)+\ \{and|or\})?\ N : N\text{--}NP0 : NP0}
\]

Since multiple and optional matching is not yet implemented, here is a version without these advanced features, that will match exactly three nouns after 'such as':

\[
\mathtt{NP\ :\ N0\ such\ as\ N1\ ,\ N2\ ,\ CC\ N3\ :\ N1\text{--}N0\ N2\text{--}N0\ N3\text{--}N0\ :\ N0}
\]

This rule applied to the sentence: "Snakes such as pythons, cobras, and boas are reptiles.", results in three edges: pythons $\rightarrow$ snakes, cobras $\rightarrow$ snakes, and boas $\rightarrow$ snakes.


\paragraph{Acknowledgments.} This work was part of the \textit{Knoex} project (abbr. knowledge
extraction from text), which again was a sub-project of the \textit{Souma} study project at the University of Osnabr√ºck. A working implementation can be downloaded from \url{http://github.com/mome/treepattern}. In the current version the postfix operator for optional and multiple matching as well as head-splitting is not implemented.

\bibliographystyle{apalike}
\bibliography{knoex.bib}

\end{document}  
