%% LyX 2.1.4 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[utf8]{inputenc}
\usepackage{babel}
\usepackage[]{algorithm2e}
\usepackage{newlfont}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{tgcursor}
\usepackage{glossaries}
\usepackage{rotating}
\usepackage{adjustbox}
\usepackage{mdframed}
\usepackage{pdflscape}
\usepackage{verbatim}
\usepackage{amsthm}
\usepackage{float}

\newtheorem*{definition}{Definition}

\graphicspath{ {images/} }
\lstset{basicstyle=\ttfamily}

\begin{document}

\title{A domain specific language for graph genenration from parse trees by pattern matching.}


\author{Moritz Meier}

\maketitle

\section{Introduction}

Due to the imense complexity and ambiguity of human language, information extraction (IE) from written natural language remains an unsolved problem. 

The DSL (Domain-Specific Language) and the underlying algorithm, described in the work, was designed to transform a phrase structure parse-tree into a graph of semantic relations. However it can in principle be used on any tree structure to produce other kinds of graph structures. The method also includes that additional propertries can be assigend to nodes of trees such that inforamtion of other sources, common in computational linguistics, can incorporated into on representation. The DSL makes it convinient to define pairs of patterns, that act locally on the tree, and corrisponding transistions to form and transform nodes and edges of a graph.
In case the reader is unfamiliar with the abbreviation used for constituents, here is a list of all tags used in the Stanford Parser: \url{http://github.com/mome/treepattern/wiki/List-of-Constituent-Tags}.

\subsection{Motivation}

If we consider natural language as a \textit{serialation of mental content}, which is decoded and encoded by humans in order to communicate. If we also define a formal language that can partially express mental content, than there must be a method which can partially map natural language to the formal language. Furtheron we can see that natural lanuage has a (mostly) modular architecture. Words can be composed syntactically to form a unit with a composed meaning. Linguistics calls this composable units \textit{constiutens} but in different theories different parts of a sententence qualify as a constituent.
Phrase-structure grammars are a common way assign a hierachical structure of constituents to a sentence. They are context-free grammars that yield a parse tree with words as terminal symboles and nodes refering to constituentns of the sentence. Despite beeing a syntactic method, phrase-structure parse trees also carry semantic information. Constituents can refer to entities or concepts, others express relations between concepts.
Computational linguistics possesses various other isolated methods to provide information about the parts of a sentence. POS-tagging finds the syntactic role of words, lemmatization removes inflection, NER-tagging assigs nouns to (semantic) categories.

\subsection{Related Work}

Pattern Matching in trees is used in various different contexts. \cite{hoffmann_pattern_1982} is a detailed discussion on pattern matching in programming language parse trees and subtree replacement for such things as code optimization and theorem proving. \cite{rim_transforming_1990} describes a computational method to transform syntactic graphs into semantic ones. The primary function of TGrep \cite{rohde_tgrep2_2004} is to extract parse trees whose structures match a specified pattern. In \cite{de_marneffe_generating_2006} dependency parse trees are generated from phrase structure parse trees.
The python package \textit{ptTools} defines a DNS for pattern matching in trees, which is intended to be applied on python parse trees for code analysis and testing. Tregex is a tool for tree querying and Tsurgen for tree manipulation \cite{levy_tregex_2006}. In \cite{ribeyre_linguistically-motivated_2012} graphs are created from dependency parse trees.

\paragraph{XPath}
The XML Path Language (or short XPath) is a query language for node selection in XML documents. The version XPath 3.0 is currently a WC3 Recommendation and in wide use. The following descriptions are respective to XPath 1.0.

\begin{comment}
Figure \ref{fig:xpath} shows elven of the thirteen different relationships that can be expressed in XPath. All relationships are defined relative to a node which is addressed by \textit{self}. On the vertical axis there is \textit{parent}, \textit{ancestor} and \textit{ancestor-or-self} in the upward direction and \textit{child}, \textit{descendent} and \textit{descendent-or-self} in the downward direction. On the horizontal axis there is \textit{preceding} and \textit{preceding-sibling} to the left and \textit{following} and \textit{following-sibling} to the right.
\end{comment}

Figure \ref{fig:xpath} visualizes the basic relationships that can be expressed
in XPath in order to construct more complex patterns. Patterns can
be matched on two axis, relative to a matching node denoted as \textit{self}.
On the vertical axis, a higher level node can be mached as \textit{ancestor},
a lower as \textit{decendant}. If the \textit{self} node should be included, \textit{ancestor-or-self} or \textit{descendent-or-self} can be used. A ancestor of first degree is a \textit{parent}
and a decendent of first degree a \textit{child}. On the horizontal
axis \textit{preceding} and \textit{following} nodes can be matched.
If a preceding or following also happens to be a sibling, it can be
matched with \textit{preceding-sibling} and \textit{following-sibling}.

\begin{figure}
\centering
\includegraphics[scale=0.35]{xpath-axis}
\caption{Subset of relationships expressable in XPath}
\label{fig:xpath}
\end{figure}


\subsection{The Imidiate-Following Relationship}

Another criterion for the language design was, beeing able to define the patterns in the same way as Hearst-Patterns \cite{hearst_automatic_1992} are defined. The first pattern from the original publication looks like this:

\[ NP_0\ such\ as\ \{NP_1,\ NP_2\ ...\ ,(and|or)\}\ NP_n \]

In this representation words and constituency tags are just put in a row seperated by whitespace. While this looks very intuitiv to a human, the technical formulation of the underlying relation, which is crucial for an actual implemntation, requieres some explanation.

\label{ssec:imidiate-following}

\begin{figure}
\centering
\includegraphics[scale=0.5]{lara_drinks_tea}
\caption{Phrase-Structure Parse Tree: Yellow nodes have no preceding nodes, blue nodes are imidiate followers of the yellow markes NP node.}
\label{fig:lara-drinks-tea}
\end{figure}


Figure \ref{fig:lara-drinks-tea} shows a phrase-structure parse tree of the sentence: "Lara drinks tea.". Lets try to match two simpler patterns in this Hearst-Pattern style on this tree:
\[NP_0\ drinks\ NP_1\ .\]
\[NP\ \ VP\ \ tea\ .\]
How to decide which one matches the sentence and which one does not?  

The nodes that can be in the beginning of a pattern are exactly the once that have no predecessor (yellow nodes in Figure \ref{fig:lara-drinks-tea}). Based on the XPath naming scheme, these nodes are said to be in a \textit{no-preceding} relation to the root node (here \textbf{S}). Both patterns would assign the yellow marked noun phrase node to their first pattern token. Once the NP node is fixed, he next possible nodes to match are: \textbf{VP}, \textbf{VBZ} or \textbf{drinks} (marked with blue). In general the constituents that can stand at the next position in a pattern hold a relation called \textit{imidiate-following} to the previous constituent.

\begin{definition}[imidiate-following]
 If a node has preceding siblings, then the first preceding sibling is imidiate-following. If a node has no preceding siblings then the first preceding sibling of the closest ancestor that has preceding siblings is imidiate-following. The first child of an imidiate-following node is imidiate-following.
\end{definition}

In the first pattern, $drinks$ would match to the \textbf{drinks} terminal node. The token $NP_1$ would consequently match to the second \textbf{NP} node and at the end the dot token to the punctuation constituent node. The whole pattern matches. In the second pattern the token $VP$ matches to the node \textbf{VP}. The set imidiate-following nodes for \textbf{VP} only contains the punctuation nodes. Hence the next token $tea$ cannot match, so the whole pattern does not match.


\section{The Domain Specific Languge}

Before the pattern matching can begin a representation of the sententence structure, called the \textit{property tree}, which incorporates different sources of informations is created. It is based on the phrase-structure parse tree with additional propertries assigned to some node. The nodes of ther property tree are also just refered to as constituents. For example, if the named-entity of a word is 'person', a new variable with name 'ner' and value 'person' can be created and added to the corresponding constituent.

After that a set of rules is loaded the way how patterns of constituents are translated into semantic domain. A rule consists of four parts, each separated by a collon: 

\[ \mathtt{root\text{-}pattern\ :\ pattern\ :\ relations\ :\ transformation} \] 

The \textit{root-pattern} is a single and the \textit{pattern} is a series of \textit{pattern-tokens}. A pattern-token defines a set of constraints in order to match a single node or in special cases multiple nodes (see Section \ref{para:multi}) of the property tree.
A node that is matched by the root-pattern will form the root node of a subtree (of the property tree). The pattern is written in a Hearst-Pattern style (just like in Section \ref{ssec:imidiate-following}) and can match constituents inside the subtree streched by the root-pattern. Constituents matched by two adjacent pattern-tokens must stand in a imidiate-following relationship. In the simplest case the pattern token can  be the label of a constituent (for example $\mathtt{NP}$) or a terminal (for example $\mathtt{drinks}$). The algorithm automatically adds 'label=NP' for constituents and 'terminal=drinks' for terminals to the constraints set. If the same name occures more than ones, numbers can be added to the token (for example $\mathtt{NP1}$). The numbers will just effect the variable name of the pattern-token, not the constraint set.  
The \textit{relations} part defines new edges of a graph. The variable names of the pattern-tokens are hereby reused. $\mathtt{NP-drinks}$ will create a directed edge between a node associated to constituent matched by $\mathtt{NP}$ and a node associated to constrituent matched by $\mathtt{drinks}$.
The \textit{transformation} part assigns a head-constrituents to the constituent matched by the root-pattern. It consists usually of one pattern-token variable name (in special cases more, see head-splitting). If another rule tries to create an edge pointing the the constituent matched by the root-pattern, it will instead point to the head-constituent.
Once all rules are parsed the constituent tags of the graph are replaced by the corresponing terminal strings.
\begin{figure}
\centering
\includegraphics[width=.6\linewidth]{step_example_parsetree}
\lstset{
  %caption=Descriptive Caption Text,
  numbers=left,
  frame=tb,
  xleftmargin=.15\textwidth, xrightmargin=.15\textwidth
} 
\begin{lstlisting}
 S  : NP VP .  : NP-VP      : S
 VP : V  NP PP : V-NP  PP-V : V
 PP : IN DT N  : DT-N  N-IN : IN 
 DT : a :: indefinite
\end{lstlisting}
\caption{Parsetree and rules for table \ref{tab:step-by-step}}
\label{fig:step-by-step}
\end{figure}
Table \ref{tab:step-by-step} gives a step-by-step example of the whole matching and graph-building process applied. The parse tree and rules hereby used are can be seen in Figure \ref{fig:step-by-step}. Coloumn one shows which constituents (bold font) mached to which pattern-token (typewriter font). Coloumn two show the varaibel graph at that point. Column two show the actual resulting graph, if the terminal assignment would already be performed at that point.

\begin{landscape}
\begin{table}
  \centering
  \begin{tabular}{|c|c|c|}
    \hline  
    Pattern Match & Graph with Constituent Labels & Graph with Terminal Labels \\ \hline \hline 

    \begin{tabular}{c||c|c|c}
      $\mathtt{S}$ & $\mathtt{NP}$ & $\mathtt{V}$ & . \\ \hline
      \textbf{S} & \textbf{NP}\textsubscript{1} & \textbf{V} & \textbf{.} 
    \end{tabular} &
    \adjustbox{valign=c}{\includegraphics[scale=0.5]{step_example_raw_1}} &
    \adjustbox{valign=c}{\includegraphics[scale=0.4]{step_example_1}} \\ \hline

    \begin{tabular}{c||c|c|c}
      $\mathtt{VP}$ & $\mathtt{V}$ & $\mathtt{NP}$ & $\mathtt{PP}$ \\ \hline
      \textbf{VP} & \textbf{VBP} & \textbf{NP}\textsubscript{2} & \textbf{PP}
    \end{tabular} &
    \adjustbox{valign=c}{\includegraphics[scale=0.5]{step_example_raw_2}} &
    \adjustbox{valign=c}{\includegraphics[scale=0.4]{step_example_2}}\\ \hline

    \begin{tabular}{c||c|c|c}
      $\mathtt{PP}$ & $\mathtt{IN}$ & $\mathtt{DT}$ & $\mathtt{N}$  \\ \hline
      \textbf{PP} & \textbf{IN} & \textbf{DT}  & \textbf{NN} 
    \end{tabular} &
    \adjustbox{valign=c}{\includegraphics[scale=0.5]{step_example_raw_3}}&
    \adjustbox{valign=c}{\includegraphics[scale=0.4]{step_example_3}}\\ \hline

    \begin{tabular}{c||c}
      $\mathtt{DT}$ & $\mathtt{a}$ \\ \hline
      \textbf{DT} & a
    \end{tabular} &
    \adjustbox{valign=c}{\includegraphics[scale=0.5]{step_example_raw_4}}&
    \adjustbox{valign=c}{\includegraphics[scale=0.4]{step_example_4}}\\ \hline
  \end{tabular}
  \caption{Step-by-step application of the rules from figure \ref{fig:step-by-step}}
  \label{tab:step-by-step}

\end{table}
\end{landscape}

\paragraph{Macros}{
  Macros are syntactic sugar for matching multiple options.  The list bilow contains all macros currently in the system:

  \begin{lstlisting}
  V  = {VB, VBD, VBG, VBN, VBP, VBZ}
  N  = {NN, NNS, NNP, NNPS, PRP}
  W  = {WHADJP, WHAVP, WHNP, WHPP}
  SS = {S, SBAR, SBARQ, SINV, SQ}
  J  = {JJ, JJR, JJS}
  \end{lstlisting}

  With \lstinline{V} standing for verbs, \lstinline{N} for nouns, \lstinline{W} for questions, \lstinline{SS} for sentences and \lstinline{J} for adjectives. They can be used just as other variables. 
}

\paragraph{Property matching}
Matching of other properties than 'terminal' or 'label' is expressed in squared brackets after the variable name. Write a variable name and a value separated by an equal sign to check if a node has a property with that name and value. The expression \lstinline{N[ner=person]} matches nouns with the NER-tag 'person'. Write only the variable name to check if a node has a propertry with that name. \lstinline{J[color]} matches adjectives with the property 'color'. Multiple constraints are separeted by a comma: \lstinline{N[ner=person | gender=female]}.
The right hand side of a property constraint can also be a set values. In this case a constitiuent must have one of these properties to match. Example: \lstinline|DT[terminal={an,a}]| - matches a determiner with terminal 'a' or 'an'.

\paragraph{Multiple and Optional parsing}
\label{para:multi}
Variables defined in pattern can occure multiple times to match multiple constituents. The relations defind for that variable are consequently applied to all matches. Example: The rule \lstinline{S : N V and V . : N-V : S} 
contains two time the variable \lstinline{V}. The rule applied to "Hanna walks and talks." will match 'walks' as well as 'talks' for \lstinline{V}. So the relation \lstinline{N-V} is formed two times, resulting in: (Hanna)$\rightarrow$ [walks], and (Hanna)$\rightarrow$ [talks].
Three postfix operators, inspiered by regular expressions, can be added to the end of a pattern token, to make the match optional or repetitive. Multiple nodes assigned to a variable are handled in same way as before.

\begin{tabular}{cll}
  $\mathbf{?}$ & optional & match once or do not match\\
  $+$ & mutliple & match at least once\\
  $*$ & multinal & match arbitrary times (inlcuding: do not match)
\end{tabular}

Example: \lstinline{NP : DT? J* N : J-N : N} matches a noun with or with or without a preceding determiner and arbitrary adjectives.


\paragraph{Cluster references}
If a sentence contains a dependent clause, one part of the sentence referece to another part of the sentence. For this purpose it is possible to specifiy a subgraph instead of a node as the head. If the root of a rule is equal to its transistions, the entities refering to the ancestors of a matching constituent will be put into a cluster. If another entity wants to point to the entity representing the root match, it will point to the cluster instead.

\paragraph{Head Splitting}
For some sentences it is desireable to let a node point to multiple other nodes. Consider this conjunction of two verb phrases: "Bob runs and cries."

\paragraph{Predicate Mode}
In predicate mode the relations part of a rule will define predicates instead of edges, with a slide changed of syntax. Example: 

\begin{lstlisting}
S : NP1 is mother of NP2 : mother_of(NP1,NP2) : S
\end{lstlisting}

\begin{landscape}
\centering
\begin{minipage}{0.45\linewidth}%
  \centering
  \begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{cluster_reference_parsetree}
  \begin{lstlisting}[frame=tb, numbers=left, keepspaces=true, xleftmargin=.1\textwidth, xrightmargin=.1\textwidth]]
SS : NP VP .? : NP-VP : SS
SS : IN SS1   :       : SS1
VP : V SS     : V-SS  : V\end{lstlisting}
  \includegraphics[width=1\linewidth]{cluster_reference_graph}
  \caption{\textbf{Cluster References.} Root and tail of line 1 are both SS: ... Line 2: "that" is ommited. Line 3: make an edge beteween the verb of the first part ("said") and the dependent clause "Lara smokes". }
  \label{fig:cluster-ref}
  \end{figure}
\end{minipage}%
\hspace{0.05\linewidth}
\begin{minipage}{0.45\linewidth}%
  \centering
  \begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{headsplit_parsetree}
  \begin{lstlisting}[frame=tb, keepspaces=true, xleftmargin=.01\textwidth, xrightmargin=.01\textwidth] 
SS : NP  VP .   : NP-VP : SS
VP : VP1 CC VP2 :       : VP1 VP2\end{lstlisting}
  \includegraphics[width=.4\linewidth]{headsplit_graph}
  \caption{\textbf{Head Splitting.} The rules in the middle applied to the parsetree on the top yield the graph in the bottom.}
  \label{fig:head-splitting}
  \end{figure}
\end{minipage}%
\end{landscape}

\section{Algorithm}

1 parse sentence
2 create property tree from parsetree
3 for each rule
  3.1 find subtrees where root node matches head constraints
  3.2 for each subtree
    3.2.1 find nodes without preceding nodes
    3.2.2 for each node match pattern

\begin{algorithm}
\SetAlgoLined
\SetKwProg{Fn}{Function}{:}{end}
\SetKwFunction{MatchPattern}{MatchPattern}
\SetKwFunction{FindMatches}{FindMatches}
\SetKwData{root}{root}
\SetKwData{head}{head}
\SetKwData{pattern}{pattern}
\SetKw{in}{in}


\Fn{\MatchPattern(\root, \head, \pattern)}
{

  \ForEach{node \in \root.descendant\_or\_self}{
    \If{node satisfies head constraints}
    {
      start\_nodes $\leftarrow$ node.no\_preceding

      assignment $\leftarrow$ empty list of length(pattern)

      matches $\leftarrow$ \FindMatches(start\_nodes, \pattern, node, assignment, index=0)  

      return matches
    }
  }

}
\end{algorithm}


\begin{algorithm}
\SetAlgoLined
\SetKwProg{Fn}{Function}{:}{end}
\SetKwFunction{MatchPattern}{MatchPattern}
\SetKwFunction{FindMatches}{FindMatches}
\SetKwData{root}{root}
\SetKwData{head}{head}
\SetKwData{pattern}{pattern}
\SetKw{in}{in}
\Fn{\FindMatches(next\_nodes, pattern, head\_node, variables, index)}
{


  \ForEach{node \in \root.descendant\_or\_self}{
    \If{node satisfies head constraints}
    {
      start\_nodes $\leftarrow$ node.no\_preceding

      assignment $\leftarrow$ empty list of length(pattern)

      matches = $\leftarrow$ \FindMatches(start\_nodes, \pattern, node, assignment) 
      
      return matches
    }
  }

}
\end{algorithm}



\section{Examples}







\subsection{Example: Hearst-Pattern}

The exact form of the first Hearst-Pattern in its original publication is:

\[
NP_0\ such\ as\ \{NP_1,\ NP_2\ ...\ ,(and|or)\}\ NP_n
\]

Expressed in the DSL this looks like:

\[
\mathtt{NP\ :\ NP0\ such\ as\ ((N\ ,)+\ \{and|or\})?\ N : N\text{--}NP0 : NP0}
\]

Since multiple and optional matching is not yet implemented, here is a version without these advanced features, that will match exactly three nouns after 'such as':

\[
\mathtt{NP\ :\ N0\ such\ as\ N1\ ,\ N2\ ,\ CC\ N3\ :\ N1\text{--}N0\ N2\text{--}N0\ N3\text{--}N0\ :\ N0}
\]

This rule applied to the sentence: "Snakes such as pythons, cobras, and boas are reptiles.", results in three edges: pythons $\rightarrow$ snakes, cobras $\rightarrow$ snakes, and boas $\rightarrow$ snakes.


\paragraph{Acknowledgements.} This work was part of the \textit{Knoex} project (abbr. knowledge
extraction from text), which again was a subproject of the \textit{SOMA} study project at the University of Osnabrück. A working implementation can be downloaded from \url{http://github.com/mome/treepattern}. In the current version the postfix operator for optional and multiple matching as well as head-splitting is not implemented.

\bibliographystyle{apalike}
\bibliography{knoex.bib}

\end{document}
